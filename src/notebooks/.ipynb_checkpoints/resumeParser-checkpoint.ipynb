{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9c2bf0",
   "metadata": {},
   "source": [
    "## Data Collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b8d1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tika import parser\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a7d714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fiko Gunawan \n",
      "Data Engineer \n",
      "\n",
      "Bandung, West Java, Indonesia • fikogunawan@outlook.com • + 6285224383844  \n",
      "linkedin.com/in/fikogunawan/ \n",
      "\n",
      " \n",
      "\n",
      "PROFESSIONAL EXPERIENCE \n",
      " \n",
      "\n",
      "TRIX STUDIO West Jakarta, Jakarta, Indonesia \n",
      "Data Engineer 2020-Present \n",
      "\n",
      "● Designed & implemented a data pipeline that processes millions of data points daily with minimum elapsed time. \n",
      "● It has created a crawler application that scraped data from the web and stored it, resulting in a decrease in manual \n",
      "\n",
      "work. \n",
      "● Successfully managed and created API to collect data from multiple sources. \n",
      "● Successfully designed & implemented an efficient data streaming pipeline using Kafka, transporting millions of messages \n",
      "\n",
      "daily. \n",
      "● Managed a team of data engineers to create data visualizations, monitored logs for errors, and identified issues before \n",
      "\n",
      "they became problems. \n",
      "● Researching and evaluating new technology to support data engineering initiatives. \n",
      "\n",
      " \n",
      "INTIKOM BERLIAN MUSTIKA Bogor, West Java, Indonesia \n",
      "Java Application Developer 2020-2020 \n",
      "\n",
      "● Designed and developed a desktop application using Java. \n",
      "● Created 10+ applications using Java to streamline processes for the company. \n",
      "● Developed a Java application for message encryption between two clients, ensuring the privacy of user information. \n",
      "\n",
      " \n",
      "ICUBE BY SIRCLO Yogyakarta, Yogyakarta, Indonesia \n",
      "Back End Engineer  2019- 2020 \n",
      "\n",
      "● Maintained a running application with different client backgrounds by implementing various code changes and \n",
      "conducting system testing. \n",
      "\n",
      "● Developed a new feature that increased customer satisfaction. \n",
      "● Developed a custom feature module used by over 1,000 customers. \n",
      "● Debugged code and identified the root cause of the issue, preventing future occurrences. \n",
      "\n",
      " \n",
      "INDOHADETAMA Bandung, West Java, Indonesia \n",
      "Web Developer 2018-2019 \n",
      "\n",
      "● Designed and developed a web application for tracking expenses that increased efficiency. \n",
      "● Monitored and maintained website speed and performance to ensure optimal user experience. \n",
      "● Maintained hospital software. \n",
      "\n",
      " \n",
      "UNIVERSITAS PENDIDIKAN INDONESIA Bandung, West Java, Indonesia \n",
      "Internet Programming Lecturer Assistant 2018-2018 \n",
      "\n",
      "● Managed a team of teaching assistants to develop course materials and create innovative assessment methods. \n",
      "● Developed a quiz module with simple test cases for each period to improve student engagement. \n",
      "● Created an interactive learning environment by incorporating group work and hands-on activities into each lesson plan. \n",
      "\n",
      " \n",
      "\n",
      "EDUCATION \n",
      " \n",
      "\n",
      "UNIVERSITAS PENDIDIKAN INDONESIA Bandung, West Java, Indonesia \n",
      "Bachelor of Computer Science 2015-2019 \n",
      "GPA: 3.60 \n",
      "Final project title: Recommendation Of SAMSAT Keliling Placement Using K-Means Clustering Algorithm \n",
      " \n",
      "SMAN 1 MAJALENGKA Majalengka, West Java, Indonesia \n",
      "Major in Science 2012-2015 \n",
      " \n",
      "\n",
      "LICENSE & CERTIFICATIONS \n",
      " \n",
      "\n",
      "● Python Certificate – HackerRank \n",
      "\n",
      "https://www.linkedin.com/in/fikogunawan/\n",
      "\n",
      "\n",
      "● Award of course completion Java Programming – Oracle \n",
      "● Learn to make an android application – Dicoding Academy \n",
      "\n",
      " \n",
      "\n",
      "SKILLS & TOOLKIT \n",
      "\n",
      "Soft skills: Leadership, Public speaking, Lobbying, Programing, Teamwork, Critical Thinking, and Problem-solving. \n",
      "Strong: Python. \n",
      "Familiar: PHP, Java, and C. \n",
      "Other: Pandas, Modin Pandas, Spark, Apache Kafka, API, Flask, SQL, NoSQL, AWS, S3, DynamoDB, EMR, Athena, Hadoop, Hive, \n",
      "\n",
      "Pentaho, Superset, Airflow, GIT, Codeigniter, Laravel, Magento, Android Programming. \n",
      " \n",
      "\n",
      "AWARDS \n",
      "\n",
      "● 1st Place Musabaqah Quran Application Design UPI \n",
      " \n",
      "\n",
      "PROJECTS \n",
      "\n",
      "● SEJAHTERA – P2P landing android apps for helping UMKM get their loan \n",
      "● XPERTIS – Information system for employee's expense report \n",
      "● Hospital Management Assets – Web application for reporting and managing the hospital's assets \n",
      "\n",
      " \n",
      "\n",
      "ORGANIZATIONS & VOLUNTEERING \n",
      "\n",
      "● REMAJA (Regenerasi Mahasiswa Majalengka) – Head of Department Communication and Information \n",
      "● KMM (Keluarga Mahasiswa Majalengka) UPI – Head of Public Relation \n",
      "● PB KEMAKOM – Leader \n",
      "● GENRE (Generation of Revolution) – Coordinator of Publication and Documentation \n",
      "● PLASA (Pelantikan Anggota Biasa) – Field Coordinator \n",
      "● MUBES (Musyawarah Besar) REMAJA – Deputy Chief Executive \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed = parser.from_file('/home/braiyenmassora/codex-telkom/resume-parser/src/data/resume.pdf')\n",
    "text = parsed[\"content\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2fb07",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for extracting skills\n",
    "\n",
    "df = pd.read_csv(\"/home/braiyenmassora/codex-telkom/resume-parser/src/notebooks/support/skill2vec-dataset-master/skill2vec_10K.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75309848",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb521eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows that have NaN/None values\n",
    "df2=df.dropna()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109fd24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = df2['bapi'].tolist()\n",
    "print('Facecream:', fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62645156",
   "metadata": {},
   "source": [
    "## Data Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962e7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create global varible to load extracted data\n",
    "extracted_text = {}\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# using matcher \n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9088728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiko Gunawan\n"
     ]
    }
   ],
   "source": [
    "# extract phone Name\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern], on_match = None)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "    \n",
    "    \n",
    "name = extract_name(text)\n",
    "print(name)\n",
    "extracted_text[\"Name\"] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a0a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fikogunawan@outlook.com']\n"
     ]
    }
   ],
   "source": [
    "# extract email \n",
    "\n",
    "import re\n",
    "def get_email_addresses(string):\n",
    "    r = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    return r.findall(string)\n",
    "\n",
    "email = get_email_addresses(text)\n",
    "print(email)\n",
    "\n",
    "extracted_text[\"E-Mail\"] = email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745222e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6285224383844']\n"
     ]
    }
   ],
   "source": [
    "# extract phone number \n",
    "\n",
    "def get_phone_numbers(string):\n",
    "    r = re.compile(r'(\\b\\d{8,}\\b)')\n",
    "    phone_numbers = r.findall(string)\n",
    "    return [re.sub(r'\\D', '', num) for num in phone_numbers]\n",
    "\n",
    "phone_number= get_phone_numbers(text)\n",
    "print(phone_number)\n",
    "\n",
    "extracted_text[\"Phone Number\"] = phone_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc07d8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Java',\n",
       " 'Flask',\n",
       " 'Software',\n",
       " 'New',\n",
       " 'Php',\n",
       " 'System',\n",
       " 'Engagement',\n",
       " 'Web',\n",
       " 'Python',\n",
       " 'Android',\n",
       " 'Developer',\n",
       " 'Apache',\n",
       " 'Data',\n",
       " 'It',\n",
       " 'User',\n",
       " 'Environment',\n",
       " 'Test',\n",
       " 'Information',\n",
       " 'Project',\n",
       " 'Oracle',\n",
       " 'Team',\n",
       " 'Sql']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract skilss\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    skills = [\"machine learning\",\n",
    "             \"deep learning\",\n",
    "             \"nlp\",\n",
    "             \"natural language processing\",\n",
    "             \"mysql\",\n",
    "             \"sql\",\n",
    "             \"django\",\n",
    "             \"computer vision\",\n",
    "              \"tensorflow\",\n",
    "             \"opencv\",\n",
    "             \"mongodb\",\n",
    "             \"artificial intelligence\",\n",
    "             \"ai\",\n",
    "             \"flask\",\n",
    "             \"robotics\",\n",
    "             \"data structures\",\n",
    "             \"python\",\n",
    "             \"c++\",\n",
    "             \"matlab\",\n",
    "             \"css\",\n",
    "             \"html\",\n",
    "             \"github\",\n",
    "              \"Apache Kafka\"\n",
    "             \"php\",\n",
    "              'wise', 'ajax', 'platinum', 'watchguard', 'capm', 'cybersecurity', 'pnl', 'atl', 'partner', 'product', 'next', 'layout', 'booting', 'team', 'optical', 'utm', 'ektron', 'amazon', 'hplc', 'nmap', 'java', 'ddr', 'contact', 'hp', 'psychology', 'ad', 'ios_client_app', 'purchasing', 'quality_assurance', 'maple', 'wif', 'microsoft', 'eim', 'demand', 'erp', 'sap', 'physical', 'failure', 'authorization', 'hlsl', 'asc', 'system', 'inventory', 'logmein', 'vehicle', 'mobile', 'pass', 'lexisnexis', 'adobe', 'netiq', 'android', 'openvz', 'ooad', 'alliance', 'telephony', 'radius', 'acquisition', 'chromatography', 'business', 'autodesk', 'adobe', 'corporate', 'rpd', 'vendavo', 'javafx', 'core', 'data', 'sql', 'memcached', 'racf', 'database', 'ada', 'serial', 'image', 'service', 'crm', 'csom', 'lso', 'sce', 'dataflux', 'ipx/spx', 'snc', 'controls', 'business', 'knowledge', 'analyst', 'netconf', 'lisa', 'lean', 'operations', 'iot', 'jad', 'it', 'system', 'sequence', 'homesite', 'java', 'maxl', 'openlayers', 'dts', 'texturing', 'hogan', 'fmla', 'cisco', 'mobile', 'gvp', 'six', 'ggsn', 'ovd', 'hdfs', 'iwms', 'business', 'bugzilla', 'openmp', 'microsoft', 'ibm', 'sap', 'middleware', 'use', 'tasktracker', 'spc', 'mfc', 'developer', 'varnish', 'security', 'unix', 'smp', 'symantec', 'ice', 'process', 'apex_classes', 'cash', 'osd', 'cfengine', 'good', 'pneumatics', 'project', 'nav', 'simulation', 'test', 'elc', 'nexpose', 'test', 'hawk', 'command', 'aers', 'cqrs', 'hard', 'spss', 'mantas', 'sap', 'allen-bradley', 'static', 'tar', 'blue_coat', 'budget', 'dfp', 'portuguese', 'glp', 'flexcube', 'oracle', 'auto', 'sql', 'cisco', 'ibm', 'strategy', 'goal', 'ffmpeg', 'microsoft', 'fmcg', 'bpos', 'capital', 'software', 'apache', 'extreme', 'sales', 'ibm', 'dam', 'itil', 'implementation', 'game', 'glsl', 'wbt', 'mobile', 'wfl', 'project', 'data', 'cfe', 'dmsii', 'data', 'radian6', 'was', 'ibm', 'sales', 'data', 'arcgis', 'japanese_language', 'sap', 'gtm', 'siprnet', 'oracle', 'java', 'software', 'rt', 'ansys', 'kaspersky', 'smartforms', 'brew', 'administrative', 'retek', 'tcm', 'novell', 'cvp', 'vim', '5s', 'certified', 'territory', 'nomad', 'mta', 'clinical_data', 'policy', 'business', 'oracle', 'msca', 'remote', 'data', 'kalido', 'jive', 'satellite', 'veritas', 'sqr', 'asm', 'profit', 'microsoft', 'chinese', 'security', 'cpm', 'product', 'query/400', 'apl', 'web', 'eds', 'uss', 'php', 'apache', 'xhtml', 'sensors', 'subcontract', 'saba', 'linkedin', 'ibm', 'emc', 'metastorm', 'motion', 'wid', 'fae', 'engagement', 'sql', 'google', 'budget', 'creativity', 'axure', 'vfd', 'tealeaf', 'fleet', 'oracle', 'oracle', 'cpsi', 'javascript', 'purchasing', 'prtg', 'fidessa', 'electronic', 'cybersecurity', 'opm', 'sass', 'snap', 'sap', 'ott', 'production', 'product', 'pdh', 'database', 'owasp', 'toc', 'adminstudio', 'monarch', 'cappuccino', 'numerical', 'activex', 'makefile', 'mapper', 'manufacturing', 'librarian', 'apache', 'solution', 'sqlite', 'xcat', 'typing', 'arcsight', 'boe', 'quality', 'software', 'java', 'ic', 'blade', 'linux+', 'medical', 'windbg', 'laboratory', 'icims', 'gcs', 'user_experience', 'nessus', 'juniper', 'elt', 'logility', 'x86', 'cygwin', 'iq', 'sem', 'hdmi', 'pivotal', 'sap', 'telephony', 'erp', 'zfs', 'isu', 'mysql', 'telemedicine', 'taxonomy', 'rf', 'bios', 'broadcast', 'security', 'bitbucket', 'clerk', 'oslo', 'tps', 'user', 'call-recording', 'industrial', 'lockbox', 'marketing', 'envi', 'data', 'frameworx', 'evpl', 'lean', 'apache', 'oracle', 'microsoft', 'information', 'gpen', 'postini', 'gdps', 'peoplesoft', 'ca', 'microsoft', 'ghost', 'microsoft', 'sabre', 'sap', 'sml', 'jbuilder', 'typescript', 'vaadin', 'cat6', 'mobile', 'snmp', 'jamf', 'environment', 'hospitality', 'sap', 'sphinx', 'kofax', 'quick', 'algorithms', 'microsoft', 'enterprise', 'fmeca', 'bonds', 'corporate_banking', 'mobile', 'led', 'rs-232', 'mp3', 'bso', 'jobvite', 'artificial', 'microsoft', 'powerbuilder', 'latam', 'semiconductors', 'gre', 'articulate', 'ibm', 'idol', 'issmp', 'personal', 'mcafee', 'virtualbox', 'sdf', 'ecl', 'business', 'dashboard', 'doctor', 'new', 'java', 'vmware', 'internal', 'backup', 'oracle', 'linear', 'ansi', 'filing', 'fit', 'wf', 'abend-aid', 'software', 'boost', 'ip', 'dcnm', 'software', 'os', 'sap', 'active', 'centricity', 'ssl', 'da', 'hl7', 'repetitive', 'apache', 'icoms', 'wiki', 'bamboo', 'security', 'visualforce', 'astrodynamics', 'bomgar', 'iaas', 'bal', 'latex', 'infor', 'electronic', 'product', 'international', 'content', 'unified', 'akka', 'start_up', 'coe', 'appdynamics', 'cisco'\n",
    "             \n",
    "             \n",
    "             ]\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n",
    "skills = []\n",
    "skills = extract_skills(text)\n",
    "\n",
    "extracted_text[\"Skills\"] = skills\n",
    "skills "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b238af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract educations\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n",
    "\n",
    "\n",
    "education = extract_education(text)\n",
    "education\n",
    "extracted_text[\"Qualification\"] = education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ef8ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sub_patterns = ['[A-Z][a-z]* University','[A-Z][a-z]* Educational Institute',\n",
    "                'University of [A-Z][a-z]*',\n",
    "                'Ecole [A-Z][a-z]*']\n",
    "pattern = '({})'.format('|'.join(sub_patterns))\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "extracted_text[\"Institutes\"] = matches\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e0c0a",
   "metadata": {},
   "source": [
    "## Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7391120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'Fiko Gunawan',\n",
       " 'E-Mail': ['fikogunawan@outlook.com'],\n",
       " 'Phone Number': ['6285224383844'],\n",
       " 'Skills': ['Java',\n",
       "  'Flask',\n",
       "  'Software',\n",
       "  'New',\n",
       "  'Php',\n",
       "  'System',\n",
       "  'Engagement',\n",
       "  'Web',\n",
       "  'Python',\n",
       "  'Android',\n",
       "  'Developer',\n",
       "  'Apache',\n",
       "  'Data',\n",
       "  'It',\n",
       "  'User',\n",
       "  'Environment',\n",
       "  'Test',\n",
       "  'Information',\n",
       "  'Project',\n",
       "  'Oracle',\n",
       "  'Team',\n",
       "  'Sql'],\n",
       " 'Qualification': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa86959",
   "metadata": {},
   "source": [
    "## Data Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312e55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6feba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868f609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703bc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83118c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2c700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ced18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

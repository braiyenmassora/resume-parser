{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7f3056",
   "metadata": {},
   "source": [
    "## Data Collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d36b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"/home/braiyenmassora/codex-telkom/resume-parser/src/notebooks/ouput/a.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[0]\n",
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feabf985",
   "metadata": {},
   "source": [
    "## Modul Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a13b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create global varible to load extracted data\n",
    "extracted_text = {}\n",
    "\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# using matcher \n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b74758",
   "metadata": {},
   "source": [
    "## Data Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e78e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract phone Name\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern], on_match = None)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "    \n",
    "    \n",
    "name = extract_name(text)\n",
    "print(name)\n",
    "extracted_text[\"fullName\"] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05b59465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def extract_portfolioLinkUrl(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [\n",
    "        {'LOWER': 'github'},\n",
    "        {'IS_PUNCT': True, 'OP': '?'},\n",
    "        {'LOWER': 'com'},\n",
    "        {'IS_PUNCT': True, 'OP': '?'},\n",
    "        {'LOWER': {'IN': ['/', 'user', 'org']}},\n",
    "        {'IS_PUNCT': True, 'OP': '?'},\n",
    "        {'LOWER': {'REGEX': '[a-zA-Z0-9_-]+'}},\n",
    "        {'IS_PUNCT': True, 'OP': '?'},\n",
    "        {'LOWER': {'REGEX': '(repos|tree|blob|issues|pull)'}, 'OP': '?'},\n",
    "        {'IS_PUNCT': True, 'OP': '?'},\n",
    "        {'LOWER': {'REGEX': '[a-zA-Z0-9/._-]+'}}\n",
    "    ]\n",
    "    matcher.add('PORTFOLIO', None, pattern)\n",
    "    matches = matcher(nlp_text)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n",
    "    return None\n",
    "\n",
    "txt = \"https://github.com/kbrajwani\"\n",
    "\n",
    "portfolioLinkUrl = extract_portfolioLinkUrl(txt)\n",
    "print(portfolioLinkUrl)\n",
    "extracted_text[\"portfolioLinkUrl\"] = portfolioLinkUrl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract email \n",
    "\n",
    "import re\n",
    "def get_email_addresses(string):\n",
    "    r = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    return r.findall(string)\n",
    "\n",
    "email = get_email_addresses(text)\n",
    "print(email)\n",
    "\n",
    "extracted_text[\"email\"] = email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_urlLinkedin(string):\n",
    "    pattern = re.compile(r'\\b(?:https?://)?(?:www\\.)?linkedin\\.com/\\S+\\b')\n",
    "    return pattern.findall(string)\n",
    "\n",
    "urlLinkedin = get_urlLinkedin(text)\n",
    "print(urlLinkedin) # prints ['linkedin.com/in/fikogunawan/']\n",
    "\n",
    "extracted_text = {}\n",
    "extracted_text[\"linkedinUrl\"] = urlLinkedin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82025d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract phone number \n",
    "\n",
    "def get_phone_numbers(string):\n",
    "    r = re.compile(r'(\\b\\d{8,}\\b)')\n",
    "    phone_numbers = r.findall(string)\n",
    "    return [re.sub(r'\\D', '', num) for num in phone_numbers]\n",
    "\n",
    "phone_number= get_phone_numbers(text)\n",
    "print(phone_number)\n",
    "\n",
    "extracted_text[\"phoneNumber\"] = phone_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bebabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract skils\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"/home/braiyenmassora/codex-telkom/resume-parser/src/notebooks/support/skills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    skills = list(data.columns.values)\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for chunk in nlp_text.noun_chunks:\n",
    "        chunk_text = chunk.text.lower().strip()\n",
    "        if chunk_text in skills:\n",
    "            skillset.append(chunk_text)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n",
    "\n",
    "skills = extract_skills(text)\n",
    "print(skills)\n",
    "\n",
    "extracted_text[\"skilss\"] = skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_web(text):\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r'\"\\b((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b\"'\n",
    "    )\n",
    "\n",
    "    # Find matches for the pattern in the parsed text\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    # Return the first match found, or None if no matches were found\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "web_url = extract_web(text)\n",
    "print(web_url)\n",
    "extracted_text[\"website\"] = web_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e064fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycld2 as cld2\n",
    "import langcodes\n",
    "\n",
    "def extract_languages(text):\n",
    "    isReliable, textBytesFound, details = cld2.detect(text)\n",
    "    languages = [lang[1] for lang in details if lang[3] > 0]  # Filter out languages with zero confidence\n",
    "    countries = [langcodes.Language(lang).language_name() for lang in languages]\n",
    "    return countries\n",
    "\n",
    "extracted_text[\"languages\"] = extract_languages(text)\n",
    "print(extracted_text[\"languages\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "file = \"/home/braiyenmassora/codex-telkom/resume-parser/src/notebooks/support/university.csv\"\n",
    "\n",
    "def extract_university(text, file):\n",
    "    df = pd.read_csv(file, header=None)\n",
    "    universities = [i.lower() for i in df[1]]\n",
    "    college_name = []\n",
    "    listex = universities\n",
    "    listsearch = [text.lower()]\n",
    "\n",
    "    for i in range(len(listex)):\n",
    "        for ii in range(len(listsearch)):\n",
    "            if re.findall(listex[i], re.sub(\" +\", \" \", listsearch[ii])):\n",
    "                college_name.append(listex[i])\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract location\n",
    "    location = \"\"\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "            location = ent.text\n",
    "            break\n",
    "\n",
    "    # Extract GPA\n",
    "    gpa = \"\"\n",
    "    for match in re.findall(r\"GPA: (\\d\\.\\d{2})\", text):\n",
    "        gpa = match\n",
    "        break\n",
    "\n",
    "    # Extract faculty and other information\n",
    "    faculty = \"\"\n",
    "    faculty_match = re.search(r\"Bachelor of [a-zA-Z ]+ \\d{4}-\\d{4}\", text)\n",
    "    if faculty_match:\n",
    "        faculty = faculty_match.group(0)\n",
    "        other_info = text[faculty_match.end() :].strip()\n",
    "\n",
    "    return college_name, location, gpa, faculty\n",
    "\n",
    "# Extract values from extract_university()\n",
    "college_name, location, gpa, faculty = extract_university(text, file)\n",
    "\n",
    "if college_name:\n",
    "    # Append the values to extracted_text\n",
    "    extracted_text[\"education\"] = {\n",
    "        \"organization\": college_name,\n",
    "        \"location\": location,\n",
    "        \"gpa\": gpa,\n",
    "        \"faculty\": faculty\n",
    "    }\n",
    "\n",
    "# Print the updated extracted_text dictionary\n",
    "print(json.dumps(extracted_text, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194c5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_work_experiences(resume_text):\n",
    "    doc = nlp(resume_text)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # Define the pattern for matching work experiences\n",
    "    pattern = [{'ENT_TYPE': 'ORG', 'OP': '?'},\n",
    "               {'ENT_TYPE': 'ADJ', 'OP': '?'}, \n",
    "               {'ENT_TYPE': 'JOB_TITLE', 'OP': '?'}, \n",
    "               {'LOWER': 'at'},\n",
    "               {'ENT_TYPE': 'ORG'},\n",
    "               {'ENT_TYPE': 'ADJ', 'OP': '?'}, \n",
    "               {'ENT_TYPE': 'DATE', 'OP': '?'}]\n",
    "\n",
    "    # Add the pattern to the matcher\n",
    "    matcher.add('EXPERIENCE', None, pattern)\n",
    "\n",
    "    # Find all matches in the document\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # Extract the relevant information from the matches\n",
    "    work_experiences = []\n",
    "    for match_id, start, end in matches:\n",
    "        work_experience = {}\n",
    "        span = doc[start:end]\n",
    "        company_name = [ent.text for ent in span.ents if ent.label_ == 'ORG']\n",
    "        job_title = [ent.text for ent in span.ents if ent.label_ == 'JOB_TITLE']\n",
    "        employment_dates = [ent.text for ent in span.ents if ent.label_ == 'DATE']\n",
    "        work_experience['company_name'] = company_name[0] if company_name else None\n",
    "        work_experience['job_title'] = job_title[0] if job_title else None\n",
    "        work_experience['employment_dates'] = employment_dates[0] if employment_dates else None\n",
    "        work_experiences.append(work_experience)\n",
    "\n",
    "    return work_experiences\n",
    "\n",
    "\n",
    "experince = extract_work_experiences(text)\n",
    "print(experince)\n",
    "extracted_text[\"experince\"] = experince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0555a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_lincence_certifications(text):\n",
    "    doc = nlp(text)\n",
    "    certifications = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"LICENSE\":\n",
    "            title = \"\"\n",
    "            organization = \"\"\n",
    "            # Example logic to extract title and organization from license entity\n",
    "            for token in ent:\n",
    "                if token.ent_type_ == \"TITLE\":\n",
    "                    title = token.text\n",
    "                elif token.ent_type_ == \"ORG\":\n",
    "                    organization = token.text\n",
    "            certifications.append({\n",
    "                \"title\": title,\n",
    "                \"issuingOrganization\": organization\n",
    "            })\n",
    "    return certifications\n",
    "\n",
    "\n",
    "\n",
    "certifications = extract_lincence_certifications(text)\n",
    "print(certifications)\n",
    "extracted_text[\"certifications\"] = certifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def extract_license_certifications(text):\n",
    "    certifications = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        tree = ne_chunk(pos_tag(word_tokenize(sent)), binary=False)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == \"S\" and \"CERTIFICATION\" in [x[0].upper() for x in subtree.leaves()]:\n",
    "                # match patterns likely to correspond to the title and organization fields\n",
    "                matches = re.findall(r'(?i)\\b(?:CERTIFICATION|LICENSE)[S]?\\b(?:\\s+IN)?\\s+([\\w.,\\s]+)\\s*(?:-|–)?\\s*([\\w.,\\s-]+)?', sent)\n",
    "                if matches:\n",
    "                    title, organization = matches[0]\n",
    "                    certifications.append({\n",
    "                        \"title\": title.strip(),\n",
    "                        \"organization\": organization.strip() if organization else ''\n",
    "                    })\n",
    "    return certifications\n",
    "\n",
    "certifications = extract_license_certifications(text)\n",
    "print({\"certifications\": certifications})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_license_cert(text):\n",
    "    certifications = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"CERTIFICATE\":\n",
    "            title = ent.text\n",
    "            organization = \"\"\n",
    "            for tok in ent.subtree:\n",
    "                if tok.ent_type_ == \"ORG\":\n",
    "                    organization += tok.text\n",
    "                else:\n",
    "                    organization += \" \"\n",
    "            certifications.append({\n",
    "                \"title\": title.strip(),\n",
    "                \"organization\": organization.strip()\n",
    "            })\n",
    "    return certifications\n",
    "\n",
    "cert = extract_license_cert(text)\n",
    "print({\"certifications\": cert})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a598504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"LICENSE  & CERTIFICATIONS Python Certificate – HackerRank\"\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append({'text': ent.text, 'label': ent.label_})\n",
    "    return entities\n",
    "\n",
    "entities = extract_entities(text)\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d77a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_certifications(resume_text):\n",
    "    doc = nlp(resume_text)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{'LOWER': {'IN': ['certification', 'license']}}, {'IS_PUNCT': True, 'OP': '?'}, {'ENT_TYPE': {'NOT_IN': ['', 'PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']}}]\n",
    "    matcher.add('CERT_ORG', None, pattern)\n",
    "    matches = matcher(doc)\n",
    "    certifications = []\n",
    "    for match_id, start, end in matches:\n",
    "        organization = doc[start:end].text\n",
    "        # extract title of certification\n",
    "        title = re.findall(r'(?i)\\b(?:CERTIFICATION|LICENSE)[S]?\\b(?:\\s+IN)?\\s+([\\w.,\\s]+)\\s*(?:-|–)?\\s*([\\w.,\\s-]+)?', doc[start:end].sent.text)\n",
    "        if title:\n",
    "            title = ' '.join(title[0])\n",
    "        else:\n",
    "            title = None\n",
    "        certifications.append({'title': title, 'organization': organization})\n",
    "    return certifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_portfolio_urls(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    portfolio_urls = []\n",
    "    for sent in nlp_text.sents:\n",
    "        for match in re.finditer(\"(?P<url>https?://[^\\s]+)\", sent.text):\n",
    "            url = match.group(\"url\")\n",
    "            if \"github\" in url or \"google drive\" in url or \"bitbucket\" in url or \"gitlab\" in url:\n",
    "                portfolio_urls.append(url)\n",
    "    return portfolio_urls\n",
    "\n",
    "resume_text = \"My Github profile can be found at https://github.com/johndoe. Check out my projects on Google Drive at https://drive.google.com/drive/folders/123. You can also find my work on Bitbucket at https://bitbucket.org/johndoe.\"\n",
    "portfolio_urls = extract_portfolio_urls(resume_text)\n",
    "print(portfolio_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2966e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c755d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175dd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e444b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6bb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d983835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50180c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dea9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
